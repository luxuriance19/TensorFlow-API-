{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported devices\n",
    "一般的系统都由多个计算devices。 TensorFlow支持CPU和GPU。 他们用字符串代表，例如：\n",
    "* \"/cpu:0\": The CPU of your machine.\n",
    "* \"/device:GPU:0\": The GPU of your machine, if you have one.\n",
    "* \"/device:GPU:1\": The second GPU of your machine, etc.\n",
    "\n",
    "如果说TensorFlow的operation在CPU和GPU上面均可以工作，那么GPU会拥有优先权。\n",
    "\n",
    "### Logging Device placement\n",
    "找到operations和tensors被分配到哪个devices，创建一个拥有 __log_device_placement__ 配置选择为 __True__ 的session。\n",
    "示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Create a graph:\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config = tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))\n",
    "\n",
    "# the output will see in the terminal:\n",
    "Device mapping:\n",
    "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K40c, pci bus\n",
    "id: 0000:05:00.0\n",
    "b: /job:localhost/replica:0/task:0/device:GPU:0\n",
    "a: /job:localhost/replica:0/task:0/device:GPU:0\n",
    "MatMul: /job:localhost/replica:0/task:0/device:GPU:0\n",
    "[[ 22.  28.]\n",
    " [ 49.  64.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual device placement\n",
    "如果不想让系统自动分配operation到device，可以通过 __with tf.device__ 创建一个device 文本，这样所有的在这个文本当中的operations都会被分配到同一个device.  \n",
    "用例如下：\n",
    "\n",
    "从这个例子的输出当中，你可以看到 a,b都被分配到 cpu:0, 由于device没有显示的指定 MatMul operation分配到哪个device，所以TensorFlow运行中会将这个operation选择一个可分配的device进行分配，并且在需要的时候自动复制这些tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Device mapping:\\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K40c, pci bus\\nid: 0000:05:00.0\\nb: /job:localhost/replica:0/task:0/cpu:0\\na: /job:localhost/replica:0/task:0/cpu:0\\nMatMul: /job:localhost/replica:0/task:0/device:GPU:0\\n[[ 22.  28.]\\n [ 49.  64.]]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a graph\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    a=tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b=tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a,b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config = tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))\n",
    "\n",
    "# the output will see in the terminal\n",
    "Device mapping:\n",
    "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K40c, pci bus\n",
    "id: 0000:05:00.0\n",
    "b: /job:localhost/replica:0/task:0/cpu:0\n",
    "a: /job:localhost/replica:0/task:0/cpu:0\n",
    "MatMul: /job:localhost/replica:0/task:0/device:GPU:0\n",
    "[[ 22.  28.]\n",
    " [ 49.  64.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allowing GPU memory growth\n",
    "默认情况下， TensorFlow把所有GPU(受制于(subject to) CUDA_VISIBLE_DEVICES)几乎全部的内存映射到process中。通过相对的使用GPU珍贵的内存可以更加有效，并且能够减少内存碎片化。\n",
    "\n",
    "在一些情况下，只需要为process分配可提供内存的一部分内容，或者是仅仅在process需要的时候增加内存的使用。TensorFlow在Session中提供两种 Config options来控制这个选择。\n",
    "\n",
    "第一个是 __allow_growth__ 选项，这个试图在依照runtime allcations来分配足够多的CPU的内存: 它首先分配非常少的内存，然后当Session运行的时候，分配所需要的更多的内存，通过TensorFlow process所需要的GPU 内存区间来增长。 注意我们并不释放内存，因为那样可能会导致更加严重的内存碎片。执行这个选择，通过 __tf.ConfigProto().gpu_options.allow_growth=True__ 来执行，详情见示例1：\n",
    "\n",
    "第二个方法是 __per_process_gpu_memory_fraction__ 选择。 这决定了可见的GPU所有内存被分配的比例。例如，你可以告诉TensorFlow分配GPU的所有内存的40%通过示例2完成：\n",
    "这种方法在你知道TensorFlow process需要多少的内存数量的时候是十分有用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config,...)\n",
    "\n",
    "# 2:\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a single GPU on a multi-GPU system\n",
    "在系统上有多个GPU，默认情况下，拥有最低ID值的GPU会被选择。如果说你想要在不同的GPU上面运行，那么，你需要显示的指定你的需求，如下面示例1：\n",
    "\n",
    "如果你指定的GPU不存在，那么你会得到一个 __InvalidArgumentError__: ，如下面示例2：\n",
    "\n",
    "如果想要 TensorFlow 自动选择一个存在的并且支持的device来运行这些operations以防止特定的device不存在，可以在创建session的时候在配置选项中设定 __allow_soft_placement=True__ ，如下面示例3："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# Creates a graph.\n",
    "with tf.device('/device:GPU:2'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))\n",
    "\n",
    "# 不存在device时，错误显示\n",
    "InvalidArgumentError: Invalid argument: Cannot assign a device to node 'b':\n",
    "Could not satisfy explicit device specification '/device:GPU:2'\n",
    "   [[Node: b = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [3,2]\n",
    "   values: 1 2 3...>, _device=\"/device:GPU:2\"]()]]\n",
    "    \n",
    "# 3：\n",
    "# Creates a graph.\n",
    "with tf.device('/device:GPU:2'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "# Creates a session with allow_soft_placement and log_device_placement set\n",
    "# to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multiple GPUs\n",
    "当在多GPUs环境下运行TensorFlow时，可以通过 multi-tower来构建模型，这里每个tower都分配给不同的GPU，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a graph.\n",
    "c = []\n",
    "for d in ['/device:GPU:2', '/device:GPU:3']:\n",
    "    with tf.device(d):\n",
    "        a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n",
    "        b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n",
    "        c.append(tf.matmul(a, b))\n",
    "with tf.device('/cpu:0'):\n",
    "    sum = tf.add_n(c)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(sum))\n",
    "\n",
    "## output of this code\n",
    "Device mapping:\n",
    "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K20m, pci bus\n",
    "id: 0000:02:00.0\n",
    "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: Tesla K20m, pci bus\n",
    "id: 0000:03:00.0\n",
    "/job:localhost/replica:0/task:0/device:GPU:2 -> device: 2, name: Tesla K20m, pci bus\n",
    "id: 0000:83:00.0\n",
    "/job:localhost/replica:0/task:0/device:GPU:3 -> device: 3, name: Tesla K20m, pci bus\n",
    "id: 0000:84:00.0\n",
    "Const_3: /job:localhost/replica:0/task:0/device:GPU:3\n",
    "Const_2: /job:localhost/replica:0/task:0/device:GPU:3\n",
    "MatMul_1: /job:localhost/replica:0/task:0/device:GPU:3\n",
    "Const_1: /job:localhost/replica:0/task:0/device:GPU:2\n",
    "Const: /job:localhost/replica:0/task:0/device:GPU:2\n",
    "MatMul: /job:localhost/replica:0/task:0/device:GPU:2\n",
    "AddN: /job:localhost/replica:0/task:0/cpu:0\n",
    "[[  44.   56.]\n",
    " [  98.  128.]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
