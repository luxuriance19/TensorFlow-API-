{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个文档得word2Vec model基于[论文](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)。这个模型用来学习词汇的矢量表达，称为\"word embeddings\"。\n",
    "\n",
    "## Highlights\n",
    "本教程旨在强调用TensorFlow创建一个word2vec模型有趣，实质的部分。\n",
    "* 首先会说明问我们想要用vectors来表达words的动机。\n",
    "* 查看模型后面的直觉(intuition)和怎么样训练的（用可以很好测量的数学：with a splash of math for measure)\n",
    "* 在TensorFlow里面怎么样简单的实现模型。\n",
    "* 最后，怎么样让原始的版本能够延伸得(scale better)更好的办法。\n",
    "在这个tutorial中会逐步查看code，但是如果说你像直接查看，可以直接查看简单实现[tensorflow/examples/tutorials/word2vec/word2vec_basic.py](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/examples/tutorials/word2vec/word2vec_basic.py)。这个简单例子包含需要下载某些数据的代码，在这个基础上训练，然后可以可视化这个结果。一旦可以查看和运行这个基础的版本，可以查看高阶版本[tutorials/embedding/word2vec.py](https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py)。在这个版本里面关于比较高阶的TensorFlow的实现，关于怎么样使用thread将数据移动导text model中间，在training的时候怎么样checkpoints等。\n",
    "但是，首先，让我们查看为什么我们想要学习word embeddings。\n",
    "\n",
    "## Motivation:Why Learn Word Embeddings?\n",
    "图像和声音处理系统一般都处理丰富的，高维度的dataset，一般用图像数据原始的像素强度(pixel-intensities)来作为vector编码，或者，例如声音数据的功率谱密度(power spectral density coefficients)系数。对于对象或者语音识别的任务，所有的需要的信息都会很好的在数据中编码（因为人可以很好的从原始的数据中处理这些任务）。但是，自然语言处理系统一般将word认为是分散的原子信号，因此，'cat'可能被表达成Id537,'dog'表达成'Id143'。这些encoding是任意的，关于他们之间的关系不能够提供任何有用的信息。这意味着这些模型当他们学习了'cats'数据，但是在处理'dogs'数据的时候，并不能够提供很大的帮助。代表的words都是唯一的，独立的id导致数据会非常的稀疏，所以这就意味着为了训练统计模型，必须需要更多的数据。使用矢量表达可以战胜这些缺点。\n",
    "![Title](../../image/audio-image-text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Vector space model](https://en.wikipedia.org/wiki/Vector_space_model)（VSM）采用连续的矢量空间来表达(embed) words，语义相似的词汇被映射到相近的点。VSMs在NLP中有很长很丰富的例是，但是所有的方法依赖于在[Distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis)的一些方法，也就是怎么样表示这些在同一个文本中的words共享语义的意思。不同的方法可以提升这个原则的方法可以被分为两大类：count-based method(例如，[latent sematic analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)),和predictive methods（例如，[Neural probabilistic language models](http://www.scholarpedia.org/article/Neural_net_language_models)。\n",
    "\n",
    "这两者的区别在[paper](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf)详细阐述。概括的说，Count-based methods计算一些词汇在一个大的文本集中和它相邻词汇共同出现的统计次数，然后将这些计算的统计数目每个词汇映射为一个小的，密集(dense)的矢量。Predictive models直接一句学习到的小的，dense embedding vectors(依照模型的参数量）来从它的相邻词汇预测词汇。\n",
    "\n",
    "Word2vec从原始的文本当中学习word embedding是非常有计算效率的预测模型。它有两种形式，连续的词袋模型(Continuous Bag-of-Word model，CBOW)和Skip-Gram model。在算法上面来说，这些模型都是相似的，除了CBOW从源文本词汇('the cat sits on the')预测目标词汇('mat'),但是skip-gram做相反的事情，它从目标的词汇来预测源文本词汇。这个倒置可能像任意的选择，但是统计上来说，他具有CBOW可以平滑很多分布信息的效果(通过将一个全部的文本认为是一个观察）。大部分情况下，这对于小的dataset来说是十分有效的。但是，skip-gram将每一个文本-目标对(context-target pair)看成是一个新的观察，当dataset更大的时候，它会表现得更好。我们在这个tutorial中会专注于skip-gram模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up with Noise-Contrastive Training\n",
    "Nerual probabilistic language models通常是用ML原则来通过给定的前面的words $h$ 来依据softmax函数来最大化下一个word $w_t$(目标）的概率。\n",
    "\\begin{align}\n",
    "P(w_t | h) &= \\text{softmax} (\\text{score} (w_t, h)) \\\\\n",
    "           &= \\frac{\\exp \\{ \\text{score} (w_t, h) \\} }\n",
    "             {\\sum_\\text{Word w' in Vocab} \\exp \\{ \\text{score} (w', h) \\} }\n",
    "\\end{align}\n",
    "在这里$\\text{score} (w_t, h)$计算词汇$w_t$和上下文$h$的兼容性(一般用点乘)。我们通过最大化它的log-likelihood来从training set上面来训练这个模型，也就是说，通过最大化\n",
    "\\begin{align}\n",
    " J_\\text{ML} &= \\log P(w_t | h) \\\\\n",
    "  &= \\text{score} (w_t, h) -\n",
    "     \\log \\left( \\sum_\\text{Word w' in Vocab} \\exp \\{ \\text{score} (w', h) \\} \\right).\n",
    "\\end{align}\n",
    "这位语言模型产生了一个合适的归一化的概率模型。虽然这开销很大(expensive)，因为需要在每个train step计算依据现在的context $h$的在词汇表$V$中的$w'$的score来归一化每个词汇的概率。\n",
    "![Title](../../image/softmax-nplm.png)\n",
    "从另一方面来说，对于在word2vec中的特征学习，我们不需要模型的所有的概率。CBOW和skip-gram 模型通过使用一个二分法对象(logistic regression)来在同一个文本中，从$k$ imaginary(noise) words$\\tilde w$中区分出实际的target $w_t$。通过下面这个图形来例句CBOW model，一个skip-gram模型就是将下面的模型直接倒过来：\n",
    "![Title](../../image/nce-nplm.png)\n",
    "从数学上来说，这个对象（对每个例子）是为了最大化：  \n",
    "$\n",
    "J_\\text{NEG} = \\log Q_\\theta(D=1 |w_t, h) +\n",
    "  k \\mathop{\\mathbb{E}}_{\\tilde w \\sim P_\\text{noise}}\n",
    "     \\left[ \\log Q_\\theta(D = 0 |\\tilde w, h) \\right]\n",
    "$     \n",
    "$Q_\\theta(D=1 | w, h)$是在这个模型当以及dataset D中从context h中能够看到word w 的二元逻辑回归的概率，根据学习当的embedding vector $\\theta$ 计算得到。在实际中，我们通过从noise distribution(也就是说，计算一个[Monte Carlo integration](Monte Carlo integration))当中选取k对比词然后估计他们的期望。\n",
    "     \n",
    "这个目标是为了让模型最大化分配高概率给实际的word，低概率给noise words。技术上来说，这叫做Negative Sampling，使用这个loss function是由一个很好的数学原因的：它提出来了一个更加近似softmax函数限制的更新。但是从计算的角度来说更加吸引人，因为这个时候我们只需要选择k个噪声词的比例而不用计算所有的词汇V当中的所有词汇。这也会让模型更容易训练。我们实际上会利用相似噪声对比估计(noise-contrastive estimation,NCE)损失，在这里TensorFlow有一个比较好处理的函数tf.nn.nce_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Skip-gram Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
