{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow根据不同的operations之间的依赖关系用一个 __dataflow graph__ 来表示计算过程。  \n",
    "这样就需要你首先定义一个dataflow graph来表示low-level programming model, 在建立一个TensorFlow的session通过一系列的local and remote devices来运行这个graph的各个部分。 \n",
    "这部分教程在你使用low-level programming model的时候非常有用。  \n",
    "Higher-level APIs，例如 __tf.estimator.Estimator__ 和 Keras 中已经将这些graph和session隐藏起来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../image/tensors_flowing.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataflow是并行计算中常用的编程模型，在dataflow graph当中，nodes代表计算单元，edges代表计算使用和产生的数据。例如，在一个TensorFlow graph当中，__tf.matmul__ operation可以对应一个带有两个输入edges，一个输出edge的单一node(a single node with two incoming edges and one outgoing edge，这两个edges代表被相乘的矩阵，输出edge代表乘法的结果) 。\n",
    "<p>\n",
    "    Dataflow被TensorFlow用来执行程序的优势:\n",
    "    <ul>\n",
    "        <li>__Parallelism__ 通过使用显示的edges来代表operations之间的相关性，这样在并行计算中，能够非常容易的被系统识别这些operations。</li>\n",
    "        <li>__Distributed execution__ 用显示的edges代表operations之间流动的数值，这样让Tensorflow可以将程序切分到不同设备上。  \n",
    "            By using explicit edges to represent the values that flow between operations, it is possible for TensorFlow to partition your program across multiple devices (CPUs, GPUs, and TPUs) attached to different machines. TensorFlow inserts the necessary communication and coordination between devices.</li>\n",
    "        <li>__Compilation__ Tensorflow 的XLA(Accelerated Linear Algebra) 编译器可以使用dataflow graph来生成更快的code，例如，融合相近的操作</li>\n",
    "        <li>__Portability__ dataflow graph 是不依赖于编码语言类型的表达。你可以通过python创建一个dataflow graph，储存为Savedmodel，然后再通过c++变成来实现low-latency inference</li>\n",
    "        </ul>\n",
    "        </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.Graph\n",
    "__tf.Graph__ 包含两部分信息：\n",
    "<ul>\n",
    "    <li>__Graph_structure__ graph的nodes和edges表示不同的operations是怎么组合在一起的，但是没有表明他们需要怎么样使用。graph structure 相当于组合code：探索它可以表达一些有用的信息，但是它并没有包含源码当中所有的有用内容。</li>  \n",
    "    <li>__Graph collections__ TensrFlow提供一个可以储存collections of metadata的一般机制，通过 __tf.Graph__ 实现。 __tf.add_to_collection__ 函数可以 通过一个key（即 __tf.GraphKeys__ 定义一些标准keys）来关联一个列表的对象； __tf.get_collection__ 通过一个 key 查询所有有关的对象。许多TensorFlow库使用这个部分： 例如，当你创建一个 __tf.Variable__, 它会默认添加到代表\"global variables\"和 \"trainable variables\"的collections当中。当你创建一个 __tf.train.Saver__ 或者 __tf.train.Optimizer__ 的时候，在这些collections当中的变量都会被用作默认的参数。</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a tf.Graph\n",
    "\n",
    "大部分的TensorFlow程序开始于图形构建阶段。在这个阶段，可以使用 TensorFlow的API 函数来构建新的 __tf.Operation__ (node) 和 __tf.Tensor__ (edge) 对象，并且将他们添加到 __tf.Graph__ 的实例当中。TensorFlow提供一个 __default graph__ ,这是左右API函数都有相同内容的一个隐式参数。例如：\n",
    "<ul>\n",
    "    <li> 调用 __tf.constant(42.0)__ 创建一个 __tf.Operation__ 产生值42.0， 将它添加到默认的graph中，并且返回一个 __tf.Tensor__ 代表这个constant的值。</li>\n",
    "    <li> 调用 __tf.matmul(x,y)__ 创建 __tf.Operation__ 让 __tf.Tensor__ 对象x和y的值相乘，添加它到默认的graph中，并且返回一个 _tf.Tensor__ 代表这个相乘结果的值</li>\n",
    "    <li> 执行 <b>v=tf.Variable(0)</b> 添加一个 <b>tf.Operation</b> 到graph, 在调用 <b>tf.Session.run__ 的时候储存一个持续可写的tensor值。 <b>tf.Variable</b> 对象包装(wrap)了这个operation，并且可以像tensor一样被使用，也就是可以读取现在被存储的值。 <b>tf.Variable</b> 对象也可以被 <b>assign,assign_add</b> 这些方法来创建 <b>tf.Operation</b> 对象，当被执行的时候，改变存储的值。</li>\n",
    "    <li> 调用 <b>tf.train.Optimizer.minimize</b> 可以将operations和tensors添加到默认的graph，并且计算梯度，返回一个 <b>tf.Operaion</b> 当执行的时候，会将这些梯度用到一系列的变量上面(apply those gradients to a set of variables).</li>\n",
    "    </ul>\n",
    "    <p>\n",
    "    大部分程序单纯的依赖于default graph，但是处理multiple graphs又多的advanced use cases。High-level APIs例如tf.estimator.Estimator API代表你操作default graph，并且，可以创建不同的training 和evaluation的graphs。\n",
    "    <p>\n",
    "    __Note:__ 调用TensorFlow API的大部分方程仅仅只是将operations个tensors添加到default graph，并不会执行真正的计算。相反的，你需要等到你有一个 __tf.Tensor__ 或者 __tf.Operation__ 能够组成这些函数代表你整个计算————例如执行梯度下降中的一步————然后将这个对象传递给 __tf.Session__ 来运行这个计算。 具体参见下面的 \"Executing a graph in a tf.Session\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naming operations\n",
    "__tf.Graph__ 对象为 __tf.Operation__ 对象定义一个命名域( __namespace__ )。 TensorFlow 自动为graph中的每个operation选择一个单独的名字。但是，给定operations的名字可以让程序更易读取和调试。TensorFlow API提供两个方法来重写operation的名字：\n",
    "<ul>\n",
    "    <li> 每个API函数创建一个新的 __tf.Operation__ 或者返回一个新的 __tf.Tensor__ 接收一个可选的 __name__ 参数。例如， __tf.constant(42.0, name=\"answer\")__ 创建了一个新的 __tf.Operation__ 命名为 __\"answer\"__ 并且返回一个 __tf.Tensor__ 命名为 __\"answer:0\"__ 。如果这个 default graph 已经包含了一个 operation命名为 __\"answer\"__ ，那么TensorFlow会在名字后面添加 __\"_1\",\"_2\"__ 等来让命名保持唯一性。</li>\n",
    "    <li> __tf.name_scope__ 函数让所有的操作在一个特殊的内容下面都能够添加一个 __name scope__ 前缀。现在的 name scope前缀是一个 \"/\"-delimited(限定) 包含在所有active __tf.name_scope__ 内容的名字列表。如果一个name scope已经存在在现有的内容当中，TensorFlow会在其后添加 __\"_1\",\"_2\"__ 等。示例如下。</li>\n",
    "    </ul>\n",
    "    <p>\n",
    "    graph visualizer 用 name scopes来 将operations组建group，从而减少graph的显示的复杂度。详情参见下面的Visualizing your graph。\n",
    "    </p>\n",
    "    <p>\n",
    "    注意 __tf.Tensor__ 对象在 __tf.Operation__ 创造了一个tensor作为输出之后会被隐式的命名，一个tensor的名字具有的格式是 __\"(OP_NAME):(i)\"__ :\n",
    "    <ul>\n",
    "        <li> __\"(OP_NAME)\"__ 是operation的名字</li>\n",
    "        <li> __\"(i)\"__ 是一个证书代表在operation输出中tensor的索引号(index)</li>\n",
    "    </ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "c_0 = tf.constant(0,name=\"c\") # => operation named \"c\"\n",
    "\n",
    "# Already-used names will be \"uniquified\"\n",
    "c_1 = tf.constant(2,name=\"c\") #=> operation named \"c_1\"\n",
    "\n",
    "# Name scopes add a prefix to all operations created in the same context.\n",
    "with tf.name_scope(\"outer\"):\n",
    "    c_2 = tf.constant(2,name=\"c\") # => operation named \"outer/c\n",
    "    \n",
    "    # Name scopes nest like paths in a hierarchical file system.\n",
    "    with tf.name_scope(\"inner\"):\n",
    "        c_3 = tf.constant(3,name=\"c\") # => operation named \"outer/inner/c\"\n",
    "        \n",
    "    # Exiting a name scope context will return to the previous prefix.\n",
    "    c_4 = tf.constant(4,name=\"c\")  # => operation named \"outer/c_1\"\n",
    "    \n",
    "    # Already-used name scopes will be \"uniquified\".\n",
    "    with tf.name_scope(\"inner\"):\n",
    "        c_5 = tf.constant(5,name=\"c\") # => operation named \"outer/inner_1/c\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### placing operations on different devices\n",
    "当TensorFlow的程序需要用到多个devices的时候， __tf.device__ 函数让所有处于一个特殊文本构成下面的所有的operations放置在同一个device或者是一种device上面。  \n",
    "\n",
    "__device specification__ 拥有以下的形势：  \n",
    "  /job:<JOB_NAME>/task:<TASK_INDEX>/device:<DEVICE_TYPE>:<DEVICE_INDEX>\n",
    "\n",
    "* <JOB_NAME> 是一个不以数字开头的 alpha-numeric 字符串\n",
    "* <DEVICE_TYPE> 是寄存device类型(例如 GPU 或者 CPU)\n",
    "* <TASK_INDEX> 是一个非负的整数代表命名为<JOB_NAME>的job的task的索引号， __tf.train.ClusterSpec__ 解释了jobs和tasks。\n",
    "* <DEVICE_INDEX> 是一个非负的整数，代表device的索引号，例如，在同一个进程(process)当中可以区分不同的GPU devices。\n",
    "定义清楚device的每一个部分是没有必要的。当运行一个只有一个GPU的设备时，只需要用 __tf.device__ 将一些任务分配给GPU和CPU。详细例子参见下面1：\n",
    "\n",
    "如果说将Tensorflow部署成 [typical distributed configuration](https://www.tensorflow.org/deploy/distributed?hl=zh-cn),你可能会将赋值(specify) job的名字和task的ID用来放置在parameter server job(__\"/job:ps\"__)里面task的变量, 和在这个worker job(__\"/job:worker\"__)里task的其他operations。参见例子2：  \n",
    "\n",
    "__tf.device__ 给放置不同operations的地方和graph区域的灵活的权限。在很多情况下，他们可以自行处理得很好。例如， __tf.train.replica_device_setter__ API 可以用到 __tf.device__ 中用来放置 operations来进行数据得并行分布训练 __data-parallel distributed training.__ 。在以下例子3：中可以看到 __tf.train.replica_device_setter__ 时怎么样操作 __tf.Variable__ 对象和其他operations得。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:\n",
    "# Operations created outside either context will run on the \"best possible\"\n",
    "# device. For example, if you have a GPU and a CPU available, and the operation\n",
    "# has a GPU implementation, TensorFlow will choose the GPU.\n",
    "weights= tf.random_normal(...)\n",
    "\n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "    # Operations created in this context will be pinned to the CPU.\n",
    "    img = tf.decode_jepg(tf.read_file(\"img.jpg\"))\n",
    "    \n",
    "with tf.device(\"/device:GPU:0\"):\n",
    "    # Operations created in this context will be pinned to the GPU.\n",
    "    result = tf.matmul(weights, img)\n",
    "    \n",
    "# 2:\n",
    "with tf.device(\"/job:ps/task:0\"):\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([784, 100]))\n",
    "    biases_1 = tf.Variable(tf.zeroes([100]))\n",
    "    \n",
    "with tf.device(\"/job:ps/task:1\"):\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([100, 10]))\n",
    "    biases_2 = tf.Variable(tf.zeroes([10]))\n",
    "\n",
    "with tf.device(\"/job:worker\"):\n",
    "    layer_1 = tf.matmul(train_batch, weights_1) + biases_1\n",
    "    layer_2 = tf.matmul(train_batch, weights_2) + biases_2\n",
    "    \n",
    "# 3:\n",
    "with tf.device(tf.train.replica_device_setter(ps_task=3)):\n",
    "    # tf.Variable objects are, by default, placed on tasks in \"/job:ps\" in a\n",
    "    # round-robin fashion.\n",
    "    w_0 = tf.Variable(...)  # placed on \"/job:ps/task:0\"\n",
    "    b_0 = tf.Variable(...)  # placed on \"/job:ps/task:1\"\n",
    "    w_1 = tf.Variable(...)  # placed on \"/job:ps/task:2\"\n",
    "    b_1 = tf.Variable(...)  # placed on \"/job:ps/task:0\"\n",
    "    \n",
    "    input_data = tf.placeholder(tf.float32)     # placed on \"/job:worker\"\n",
    "    layer_0 = tf.matmul(input_data, w_0) + b_0  # placed on \"/job:worker\"\n",
    "    layer_1 = tf.matmul(layer_0, w_1) + b_1     # placed on \"/job:worker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor-like objects\n",
    "许多TensorFlow operations可以将一个或者多个 __tf.Tensor__ 对象作为参数， 例如 __tf.matmul__ 将两个 __tf.Tensor__ 对象作为参数， __tf.add_n__ 将一个列表的 __tf.Tensor__ 对象作为参数。 简便而言，这些函数会接收一个 __tensor_like 对象__ 替代 __tf.Tensor__ ，并且将其隐式用 __tf.convert_to_tensor__ 方法将其转换为 __tf.Tensor__ 。Tensor—like 对象包含以下的元素类型：\n",
    "* tf.Tensor\n",
    "* tf.Variable\n",
    "* numpy.ndarray\n",
    "* list ( and lists of tensor-like objects)\n",
    "* Scalar Python types： bool, float, int, str\n",
    "也可以通过 __tf.register_tensor_conversion_function__ 来登记另外的tensor-like types。\n",
    "\n",
    "> __Note:__ 默认情况下，每次当你同样的tensor—like object的时候，TensorFlow都会创建一个新的 __tf.Tensor__ 。如果这个 tensor-like object 非常大的时候（例如 __numpy.ndarrary__ 包含一系列的训练样本），并且你还会多次使用，那么你可能会内存耗尽。 为了防止这件事情的发生，应当手动的调用 __tf.convert_to_tensor__ 将tensor-like对象转换一次，再使用其返回的 __tf.Tensor__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing a graph in a tf.Session\n",
    "TensorFlow用 __tf.Session__ 类来表示用户程序(client program)之间的连接——一般而言时python程序，但是也有相似的接口提供给C++程序。 一个 __tf.Session__ 对象提供进入本地machine中的devices和通过用分布的TensorFlow运行(runtime)的远程devices。它还可以缓存 __tf.Graph__ 这样就可以有效的多次运行相同的计算。\n",
    "#### Creating a tf.Session\n",
    "如果用一个low-level TensorFlow API， 你可以为 default graph创建一个 __tf.Session__ 如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a default in-process session.\n",
    "with tf.Session() as sess:\n",
    "  # ...\n",
    "\n",
    "# Create a remote session.\n",
    "with tf.Session(\"grpc://example.org:2222\"):\n",
    "  # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 __tf.Session__ 拥有一个物理资源(类似GPUs和网络连接), 它可以被用作内容管理(in a __with__ block)，并且可以在你不在这个区快的时候关闭这个session。 也可以创建一个session不通过 __with__ 块。但是必须要在结束这个session释放资源的时候显示的调用 __tf.Session.close__.\n",
    "> __Note:__ Higher-level APIs 例如 __tf.train.MonitoredTrainingSession__ 或 __tf.estimator.Estimator__ 会自动创建和管理一个 __tf.Session__. 这些APIs接受选择性的 __target__ 和 __config__ 参数（要么直接，要么作为一个 __tf.estimator.Runconfig__ 对象），与下面的描述具有同样的意义。\n",
    "\n",
    "__tf.Session.__init__ 接收三个可选择的参数：\n",
    "* __target__ 如果这个参数时空的（默认），那么这个session只会用本地machine中的devices。但是，你可能会specify 一个 __grpc://__ URL来specify一个TensorFlow server的地址，这让session可以进入这个server控制的所有的devices。 查看 [tf.train.Server](https://www.tensorflow.org/api_docs/python/tf/train/Server?hl=zh-cn)可以知道怎么样来创建一个TensorFlow server。例如，在平常的 __between-graph replication__ 配置当中， __tf.Session__  与用户在同一个process当中连接到一个 __tf.train.Server__ 。分布式Tensorflow 配置说明当中描述的其他的常见场景。\n",
    "* __graph__ 默认情况下，一个新的 __tf.Session__ 会被局限在——并且只能够运行operations——在现有的default graph中。如果你在程序中用multiple graph(see Programming with multiple graphs for more details),你可以在构建session的时候显示说明 __tf.Graph__ 。 \n",
    "* __config__ 这个参数让你指定一个 __tf.ConfigProto__ 用来控制session的行为。例如一些configuration选择包括：\n",
    "  * __allow_soft_placement__ 将这个设置为 __True__ 可以运行一个\"soft\" device 放置算法，这样会忽略 __tf.device__ 试图将只能放置在CPU上面的operations放置在GPU上的标注忽略，并且将他们放置在CPU上面。（which ignores tf.device annotations that attempt to place CPU-only operations on a GPU device, and places them on the CPU instead.）\n",
    "  * __cluster_def__ 当用分布式的 TensorFlow，这个选项让你可以制定那个machine用来进行计算，并且提供一个在job名字，task索引号和网络地址之间的mapping。通过 [tf.train.ClusterSpec.as_cluster_def](https://www.tensorflow.org/api_docs/python/tf/train/ClusterSpec?hl=zh-cn#as_cluster_def)参见详情。\n",
    "  * __graph_options.optimizer_options__ 提供在graph上面执行时的优化控制\n",
    "  * __gpu_options.allow_growth__ 将这个设置为 __True__ 可以改变GPU的内存allocator，可以逐渐增加被分配的内存数量而不是一开始就分配最大的内存数量。\n",
    "  \n",
    "#### Using tf.Session.run to execute operations\n",
    "__tf.Session.run__ 方法是运行 __tf.Operation__ 和评估 __tf.Tensor__ 最主要的机制。可以通过 __tf.Session.run__ 来传递一个或者多个的 __tf.Operation__ 和 __tf.Tensor__ 对象，并且TensorFlow会执行这些需要计算结果的oprations。  \n",
    "  \n",
    "__tf.Session.run__ 要求你制定一个列表的 __fetches__ ，用来决定返回值，或者是 __tf.Operation__ ， __tf.Tensor__  或 __tensor-like type__ 例如 __tf.Variable__ 。 这些 __fetches__ 决定在全部的 __tf.Graph__ 中哪些 __子图__ 需要被执行来产生结果。 这些子图包含在 fetch list中的所有的operations， 外加所有operations的输出是用来计算这些fetches的值的。例如，以下的代码中表明怎么样用 不同的参数来控制 __tf.Session.run__ 来产生不同的子图来被执行：示例如1：\n",
    "\n",
    "__tf.Session.run__ 也可以选择性的选择一个字典作为输入(a dictionary of feeds)， 是将 __tf.Tensor__ 对象(一般而言是 __tf.placeholdr__ tensors) 映射到值(一般而言是 python scalars,lists,or numpy arrays)可以在执行的时候替换这些tensor，示例如2：\n",
    "\n",
    "__tf.Session.run__ 一个可选择的 __options__ 参数让你可以指定调用的选择，可选择的参数 __run_metadata__ 参数可以让你收集在执行时的metadata。 例如，你可以用以下的选择在执行的时候收集轨迹信息(tracing information)。如示例3："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[node {\n",
      "  name: \"MatMul/a\"\n",
      "  op: \"Const\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_FLOAT\n",
      "        tensor_shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        tensor_content: \"\\000\\000\\024B\\000\\000\\270\\301\\000\\000\\200?\\000\\000\\200@\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"random_uniform/shape\"\n",
      "  op: \"Const\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_INT32\n",
      "        tensor_shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        tensor_content: \"\\002\\000\\000\\000\\002\\000\\000\\000\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"random_uniform/RandomUniform\"\n",
      "  op: \"RandomUniform\"\n",
      "  input: \"random_uniform/shape\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"seed\"\n",
      "    value {\n",
      "      i: 0\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"seed2\"\n",
      "    value {\n",
      "      i: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"random_uniform/sub\"\n",
      "  op: \"Const\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_FLOAT\n",
      "        tensor_shape {\n",
      "        }\n",
      "        tensor_content: \"\\000\\000\\200?\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"random_uniform/mul\"\n",
      "  op: \"Mul\"\n",
      "  input: \"random_uniform/RandomUniform\"\n",
      "  input: \"random_uniform/sub\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"random_uniform/min\"\n",
      "  op: \"Const\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_FLOAT\n",
      "        tensor_shape {\n",
      "        }\n",
      "        float_val: 0.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"random_uniform\"\n",
      "  op: \"Add\"\n",
      "  input: \"random_uniform/mul\"\n",
      "  input: \"random_uniform/min\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"MatMul\"\n",
      "  op: \"MatMul\"\n",
      "  input: \"MatMul/a\"\n",
      "  input: \"random_uniform\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"transpose_a\"\n",
      "    value {\n",
      "      b: false\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"transpose_b\"\n",
      "    value {\n",
      "      b: false\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"_retval_MatMul_0_0\"\n",
      "  op: \"_Retval\"\n",
      "  input: \"MatMul\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"index\"\n",
      "    value {\n",
      "      i: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "library {\n",
      "}\n",
      "versions {\n",
      "  producer: 24\n",
      "}\n",
      "]\n",
      "dev_stats {\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  node_stats {\n",
      "    node_name: \"_SOURCE\"\n",
      "    all_start_micros: 1516980124131896\n",
      "    op_start_rel_micros: 253\n",
      "    op_end_rel_micros: 254\n",
      "    all_end_rel_micros: 422\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    timeline_label: \"_SOURCE = NoOp()\"\n",
      "    scheduled_micros: 1516980124131855\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"MatMul/a\"\n",
      "    all_start_micros: 1516980124132599\n",
      "    op_start_rel_micros: 2\n",
      "    op_end_rel_micros: 3\n",
      "    all_end_rel_micros: 6\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_FLOAT\n",
      "        shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 16\n",
      "          allocator_name: \"cpu\"\n",
      "          ptr: 1839701595072\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"MatMul/a = Const()\"\n",
      "    scheduled_micros: 1516980124132318\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"random_uniform/shape\"\n",
      "    all_start_micros: 1516980124132608\n",
      "    all_end_rel_micros: 1\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_INT32\n",
      "        shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 8\n",
      "          allocator_name: \"cpu\"\n",
      "          ptr: 1839701595392\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"random_uniform/shape = Const()\"\n",
      "    scheduled_micros: 1516980124132605\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"random_uniform/sub\"\n",
      "    all_start_micros: 1516980124132613\n",
      "    op_start_rel_micros: 1\n",
      "    op_end_rel_micros: 1\n",
      "    all_end_rel_micros: 2\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_FLOAT\n",
      "        shape {\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 4\n",
      "          allocator_name: \"cpu\"\n",
      "          ptr: 1839701597184\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"random_uniform/sub = Const()\"\n",
      "    scheduled_micros: 1516980124132609\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"random_uniform/min\"\n",
      "    all_start_micros: 1516980124132616\n",
      "    op_start_rel_micros: 1\n",
      "    op_end_rel_micros: 1\n",
      "    all_end_rel_micros: 1\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_FLOAT\n",
      "        shape {\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 4\n",
      "          allocator_name: \"cpu\"\n",
      "          ptr: 1839701600064\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"random_uniform/min = Const()\"\n",
      "    scheduled_micros: 1516980124132615\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"random_uniform/RandomUniform\"\n",
      "    all_start_micros: 1516980124132619\n",
      "    op_end_rel_micros: 309\n",
      "    all_end_rel_micros: 313\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "      total_bytes: 16\n",
      "      peak_bytes: 16\n",
      "      live_bytes: 16\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_FLOAT\n",
      "        shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 16\n",
      "          allocated_bytes: 16\n",
      "          allocator_name: \"cpu\"\n",
      "          allocation_id: 1\n",
      "          has_single_reference: true\n",
      "          ptr: 1839701585472\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"random_uniform/RandomUniform = RandomUniform(random_uniform/shape)\"\n",
      "    scheduled_micros: 1516980124132610\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"random_uniform/mul\"\n",
      "    all_start_micros: 1516980124132934\n",
      "    op_start_rel_micros: 1\n",
      "    op_end_rel_micros: 333\n",
      "    all_end_rel_micros: 338\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_FLOAT\n",
      "        shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 16\n",
      "          allocated_bytes: 16\n",
      "          allocator_name: \"cpu\"\n",
      "          allocation_id: 1\n",
      "          ptr: 1839701585472\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"random_uniform/mul = Mul(random_uniform/RandomUniform, random_uniform/sub)\"\n",
      "    scheduled_micros: 1516980124132932\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"random_uniform\"\n",
      "    all_start_micros: 1516980124133274\n",
      "    op_start_rel_micros: 1\n",
      "    op_end_rel_micros: 649\n",
      "    all_end_rel_micros: 666\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_FLOAT\n",
      "        shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 16\n",
      "          allocated_bytes: 16\n",
      "          allocator_name: \"cpu\"\n",
      "          allocation_id: 1\n",
      "          ptr: 1839701585472\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"random_uniform = Add(random_uniform/mul, random_uniform/min)\"\n",
      "    scheduled_micros: 1516980124133272\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"MatMul\"\n",
      "    all_start_micros: 1516980124133942\n",
      "    op_start_rel_micros: 1\n",
      "    op_end_rel_micros: 3795\n",
      "    all_end_rel_micros: 3805\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "      total_bytes: 16\n",
      "      peak_bytes: 16\n",
      "      live_bytes: 16\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_FLOAT\n",
      "        shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 16\n",
      "          allocated_bytes: 16\n",
      "          allocator_name: \"cpu\"\n",
      "          allocation_id: 1\n",
      "          has_single_reference: true\n",
      "          ptr: 1839701584768\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"MatMul = MatMul(MatMul/a, random_uniform)\"\n",
      "    scheduled_micros: 1516980124133940\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"_retval_MatMul_0_0\"\n",
      "    all_start_micros: 1516980124137751\n",
      "    op_start_rel_micros: 3\n",
      "    op_end_rel_micros: 5\n",
      "    all_end_rel_micros: 8\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    timeline_label: \"_retval_MatMul_0_0 = _Retval(MatMul)\"\n",
      "    scheduled_micros: 1516980124137747\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1：\n",
    "x = tf.constant([[37.0, -23.0], [1.0, 4.0]])\n",
    "w = tf.Variable(tf.random_uniform([2, 2]))\n",
    "y = tf.matmul(x, w)\n",
    "output = tf.nn.softmax(y)\n",
    "init_op = w.initializer\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer on `w`.\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # Evaluate `output`. `sess.run(output)` will return a NumPy array containing\n",
    "    # the result of the computation.\n",
    "    print(sess.run(output))\n",
    "\n",
    "    Evaluate `y` and `output`. Note that `y` will only be computed once, and its\n",
    "    # result used both to return `y_val` and as an input to the `tf.nn.softmax()`\n",
    "    # op. Both `y_val` and `output_val` will be NumPy arrays.\n",
    "    y_val, output_val = sess.run([y, output])\n",
    "    \n",
    "# 2:\n",
    "# Define a placeholder that expects a vector of three floating-point values,\n",
    "# and a computation that depends on it.\n",
    "x = tf.placeholder(tf.float32, shape=[3])\n",
    "y = tf.square(x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Feeding a value changes the result that is returned when you evaluate `y`.\n",
    "    print(sess.run(y,{x:[1.0, 2.0, 3.0]})) # => \"[1.0, 4.0, 9.0]\"\n",
    "    print(sess.run(y, {x: [0.0, 0.0, 5.0]})  # => \"[0.0, 0.0, 25.0]\"\n",
    "          \n",
    "    # Raises `tf.errors.InvalidArgumentError`, because you must feed a value for\n",
    "    # a `tf.placeholder()` when evaluating a tensor that depends on it.\n",
    "    sess.run(y)\n",
    "          \n",
    "    # Raises `ValueError`, because the shape of `37.0` does not match the shape\n",
    "    # of placeholder `x`.\n",
    "    sess.run(y, {x: 37.0})\n",
    "          \n",
    "# 3:\n",
    "y = tf.matmul([[37.0, -23.0], [1.0, 4.0]], tf.random_uniform([2, 2]))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Define options for the `sess.run()` call.\n",
    "    options = tf.RunOptions()\n",
    "    options.output_partition_graphs = True\n",
    "    options.trace_level = tf.RunOptions.FULL_TRACE\n",
    "\n",
    "    # Define a container for the returned metadata.\n",
    "    metadata = tf.RunMetadata()\n",
    "\n",
    "    sess.run(y, options=options, run_metadata=metadata)\n",
    "\n",
    "    # Print the subgraphs that executed on each device.\n",
    "    print(metadata.partition_graphs)\n",
    "\n",
    "    # Print the timings of each operation that executed.\n",
    "    print(metadata.step_stats)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing your graph\n",
    "TensorFLow 包含可以帮助你理解一个 graph 的工具。 __graph visualizer__ 是TensorBoard的组件让graph的结构在浏览器里面显示。最简单的创建一个可视化的方法是创建 __tf.summary.FileWriter__ 传递一个 __tf.Graph__ .\n",
    "> __Note:__ 如果用 __tf.estimator.Estimator__ ，graph（或者是任意的summaries)都可以被自动记录（logged)到 __model_dir__ ，这样就可以在创建estimator的时候被specified。\n",
    "\n",
    "在 __tensorboard__ 中也可以打开一个log， 用来导航到\"Graph\" tab, 而且可以看到graph结构的high-level可视化。 注意，一般而言tensorflow graph-- 特别是带有自动计算梯度的training graph--一次需要显示特别多的nodes。graph visualizer 利用name scopes将这些相关的operations group到一个\"super\" nodes中。 点击黄色的+键就可以将这些super nodes展开成它里面的子图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your graph.\n",
    "x = tf.constant([[37.0, -23.0], [1.0, 4.0]])\n",
    "w = tf.Variable(tf.random_uniform([2, 2]))\n",
    "y = tf.matmul(x, w)\n",
    "# ...\n",
    "loss = ...\n",
    "train_op = tf.train.AdagradOptimizer(0.01).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # `sess.graph` provides access to the graph used in a `tf.Session`.\n",
    "    writer = tf.summary.FileWriter(\"/tmp/log/...\", sess.graph)\n",
    "\n",
    "    # Perform your computation...\n",
    "    for i in range(1000):\n",
    "        sess.run(train_op)\n",
    "        # ...\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../image/mnist_deep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming with multiple graphs\n",
    "> __Note:__ 当训练一个model，最通常的做法是用一个code描述一个graph来training模型，还有分开的graph用来evaluating或者执行训练模型的运行推断(performing inference with a trained model)。在大部分请款管辖，inference graph是不同于training graph的：例如，像dropout和batch normalization的技术会在不同的情况在回采用不同的operations。更多的是，像 __tf.train.Saver__ 这样的工具包使用 __tf.Variable__ 对象的名字（他们的名字都是基于 __tf.Operation__ ）来认证被储存的checkpoint的不同的变量。编程时，你可以完全使用分开的Python processes来创建和执行graphs，也可以在同一个process中使用multi graph。以下将介绍怎么样在一个process当中使用 multi graph。\n",
    "\n",
    "在前面提出，TensorFlow 提供一个 \"default graph\", 可以在一个context下隐式的传递所有的API functions。在许多的清醒下，一个graph是足够的，但是，TensorFlow也提供一些方法来操控default graph，在更加adavanced情形下会更加有用。例如：\n",
    "* __tf.Graph__ 定义 __tf.Operation__ 对象的命名空间，在一个graph中每个operation必须具有单独的名字。 TensorFlow会通过在operations的名字后面添加 __\"_1\",\"_2\"__  等等来让名字单一化。使用多个显示的创建的graphs让你在每个operation上的命名有更大的控制权限。\n",
    "* default graph会储存每个曾经添加到它的 __tf.Operation__ 和 __tf.Tensor__ 的信息。 如果说在程序当中有非常多的没有连接的子图， 那么通过 __tf.Graph__ 创建不同的子图会更有效，那么这些不相关的state就可以被垃圾回收了。\n",
    "\n",
    "你可以像default graph一样安装(install)一个不同的 __tf.Graph__ ，在context manager 中使用 __tf.Graph.as_default__ ,例如示例1：\n",
    "\n",
    "探查(inspect)现在的default graph，可以通过调用 __tf.get_default_graph__ ，返回一个 __tf.Graph__ object, 例如示例2：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:\n",
    "g_1 = tf.Graph()\n",
    "with g_1.as_default():\n",
    "    # Operations created in this scope will be added to `g_1`.\n",
    "    c = tf.constant(\"Node in g_1\")\n",
    "    \n",
    "    # Sessions created in this scope will run operations from `g_1`.\n",
    "    sess_1 = tf.Session()\n",
    "    \n",
    "g_2 = tf.Graph()\n",
    "with g_2.as_default():\n",
    "    # Operations created in this scope will be added to `g_2`.\n",
    "    d = tf.constant(\"Node in g_2\")\n",
    "    \n",
    "# Alternatively, you can pass a graph when constructing a `tf.Session`:\n",
    "# `sess_2` will run operations from `g_2`.\n",
    "sess_2 = tf.Session(graph=g_2)\n",
    "\n",
    "assert c.graph is g_1\n",
    "assert sess_1.graph is g_1\n",
    "\n",
    "assert d.graph is g_2\n",
    "assert sess_2.graph is g_2\n",
    "\n",
    "# 2:\n",
    "# Print all of the operations in the default graph\n",
    "g = tf.get_default_graph()\n",
    "print(g.get_operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Graph.get_operations of <tensorflow.python.framework.ops.Graph object at 0x000001AC52AA1C88>>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
