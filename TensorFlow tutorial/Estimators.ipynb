{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators 封装在下列actions当中：\n",
    "<ul type='disc'>\n",
    "    <li>training</li>\n",
    "    <li>evaluation</li>\n",
    "    <li>prediction</li>\n",
    "    <li>export for serving</li>\n",
    "</ul>\n",
    "<font size=\"2\" color=\"grey\"> 所有的Estimators都基于tf.estimator.Estimators 类(tf.contrib.learn.Estimator)已经被废弃，不应该继续使用</font>   \n",
    "\n",
    "<b> Estimator 优势：</b>\n",
    "\n",
    "<ul>\n",
    "    <li>使用Estimator_based模型，不需要在不同的平台上修改代码</li>\n",
    "    <li>Estimators simplify sharing implementations between model developers</li>\n",
    "    <li>You can develop a state of the art model with high-level intuitive code, In short, it is generally much easier to create models with Estimators than with the low-level TensorFlow APIs.</li>\n",
    "    <li>Estimators 简历在tf.layers上面，可以简单的被自定义</li>\n",
    "    <li>Estimators可以自动的创建Graph</li>\n",
    "    <li>Estimators可以提供safe distributed training loop that controls how and when to:</li>\n",
    "    <ul>\n",
    "    <li>构建图形</li>\n",
    "    <li>初始化变量</li>\n",
    "    <li> start queues</li>\n",
    "    <li>处理已成</li>\n",
    "    <li>create checkpoint files and recover from failures</li>\n",
    "    <li>为Tensorboard保存summaries</li>\n",
    "    </ul>\n",
    "    </ul>  \n",
    "当写一个具有Estimators的应用是，you must separate the data input pipeline from the model.这样做可便于实验数据的替换。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-made Estimators\n",
    "pre-made Estimators可以创建和管理Graph和Session对象。   \n",
    "Furthermore, pre-made Estimators let you experiment with different model architectures by making only minimal code changes. DNNClassifier, for example, is a pre-made Estimator class that trains classification models through dense, feed-forward neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>tf.estimator.LinearClassifier:</b> Constructs a linear classification model.\n",
    "<b>tf.estimator.LinearRegressor:</b> Constructs a linear regression model.  \n",
    "<b>tf.estimator.DNNClassifier:</b> Construct a neural network classification model.  \n",
    "<b>tf.estimator.DNNRegressor:</b> Construct a neural network regression model.  \n",
    "<b>tf.estimator.DNNLinearCombinedClassifier:</b> Construct a neural network and linear combined classification model.  \n",
    "<b>tf.estimator.DNNLinearCombinedRegressor:</b> Construct a neural network and linear combined regression model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of a pre-made Estimators program\n",
    "1.<b> 写一个或者多个dataset导入函数。</b>  \n",
    "input_fn\n",
    "<p> 每个dataset导入函数返回两个对象：\n",
    "    <ul>\n",
    "        <li> 字典：keys：特征名称，values：Tensor(or sparse Tensros)对应相应的特征。</li>\n",
    "        <li> Tensor: 包含一个或者多个标签</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>\n",
    "    input function(input_fn)的主题包含了input data预处理过程，必须要返回两个值包含处理过的feature和label(用于传递给模型)：（函数的skeleton见上面)my_input_fn):  \n",
    "    <ul>\n",
    "        <li><b>feature_cols: </b>dict: 包含key/value成对对应到feature columns names和包含相应特征的Tensor(Sparse Tensor)</li>\n",
    "        <li><b>labels: </b>a Tensor包含目标值(label)，即模型的预测值</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>\n",
    "    如果feature/label data使用python array存储在pandas dataframe或者numpy array中，可以用以下方法来构建input_fn:详细例子如下\n",
    "</p>\n",
    "<p>\n",
    "    对于sparse,categorical data(大部分值为0),那么input_fn可以改为填充一个SparseTensor，可以通过三个参数实例化：\n",
    "    <ul>\n",
    "        <li><b>dense_shape: </b>  \n",
    "            the shape of tensor.list类型，每一个元素代表当前的维度，e.g.dense_shape=[3,6],[2,3,4],[9]分别代表二维，三维，一维的tensor\n",
    "        </li>\n",
    "        <li><b>indices: </b>  \n",
    "            知名非零值出现的位置，list类型。index以0开始，即[0,0]表示第一行第一列，indices=[[1,3],[2,4]]代表[1,3],[2,4]的index处不为零\n",
    "        </li>\n",
    "        <li><b>values: </b>  \n",
    "            一维tensor，第i个值表示indices中的第i个元素填充的值\n",
    "        </li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#e.g.: \n",
    "def input_fn(dataset):\n",
    "    ...# manipulate dataset, extracting feature names and the label\n",
    "    return feature_dict,label\n",
    "\n",
    "def my_input_fn():\n",
    "    # Preprocess your data here...\n",
    "\n",
    "    # ...then return 1) a mapping of feature columns to Tensors with\n",
    "    # the corresponding feature data, and 2) a Tensor containing labels\n",
    "    return feature_cols, labels\n",
    "\n",
    "import numpy as np\n",
    "# numpy input_fn.\n",
    "my_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(x_data)},\n",
    "    y=np.array(y_data),\n",
    "    ...)\n",
    "\n",
    "import pandas as pd\n",
    "# pandas input_fn.\n",
    "my_input_fn = tf.estimator.inputs.pandas_input_fn(\n",
    "    x=pd.DataFrame({\"x\": x_data}),\n",
    "    y=pd.Series(y_data),\n",
    "    ...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5]\n"
     ]
    }
   ],
   "source": [
    "sparse_tensor = tf.SparseTensor(indices=[[0,1], [2,4]],\n",
    "                                values=[6, 0.5],\n",
    "                                dense_shape=[3, 5])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(sparse_tensor).dense_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.<b> 定义特征columns。</b>每一个tf.feature_column定义(identifies)一个特征名字，他的类型和任何的预处理（input pre-processing)  \n",
    "<p>这个例子中创建了三个特征columns包含integer或者是float-point数据\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define three numeric feature columns\n",
    "population = tf.feature_column.numeric_column('population')\n",
    "crime_rate = tf.feature_column.numeric_column('crime_rate')\n",
    "median_education = tf.feature_column.numeric_column('median_edcation',\n",
    "                                                     normalizer_fn='lambda x:x-global_education_mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.<b> 实例化相关的pre-made Estimator。</b>\n",
    "<p>这个例子中实例化pre-made Estimator为LinearClassifier\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an estimator, passing the feature columns.\n",
    "estimator = tf.estimator.Estimator.LinearClassifier(\n",
    "    feature_columns=[population,crime_rate,median_education],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.<b> Call a training, evaluation, or inference method。</b>\n",
    "<p>所有的Estimator都提供一个train模型\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_training_set is the function created in Step 1\n",
    "estimator.train(input_fn=my_training_set,steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of pre-made Estimators\n",
    "Pre-made Estimators encode best practices, providing the following benefits:  \n",
    "<ul>\n",
    "    <li>Best practices for determining where different parts of the computational graph should run, implementing strategies on a single machine or on a cluster.  \n",
    "    （决定计算图形哪些不同的地方应当运行，在一个机器或者一个cluster上实行决策的最好的实现）\n",
    "    </li>\n",
    "    <li>Best practices for event (summary) writing and universally useful summaries.（event写定和普遍有用的总结的实现）</li>\n",
    "</ul>\n",
    "如果不适用pre-made Estimators,那么需要自行实践preceding features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pre-made Estimator 实例：DNNclassifier，实行dense，feed-forward NN\n",
    "\n",
    "categorical_feature_a = categorical_column_with_hash_bucket(...)\n",
    "categorical_feature_b = categorical_column_with_hash_bucket(...)\n",
    "\n",
    "categorical_feature_a_emb = embedding_column(\n",
    "    categorical_column=categorical_feature_a, ...)\n",
    "categorical_feature_b_emb = embedding_column(\n",
    "    categorical_column=categorical_feature_b, ...)\n",
    "\n",
    "estimator = DNNClassifier(\n",
    "    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n",
    "    hidden_units=[1024, 512, 256])\n",
    "\n",
    "# Or estimator using the ProximalAdagradOptimizer optimizer with\n",
    "# regularization.\n",
    "estimator = DNNClassifier(\n",
    "    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n",
    "    hidden_units=[1024, 512, 256],\n",
    "    optimizer=tf.train.ProximalAdagradOptimizer(\n",
    "      learning_rate=0.1,\n",
    "      l1_regularization_strength=0.001\n",
    "    ))\n",
    "\n",
    "# Input builders\n",
    "def input_fn_train: # returns x, y\n",
    "    pass\n",
    "    #e.g.  \n",
    "     return tf.estimator.inputs.pandas_input_fn(\n",
    "      x=pd.DataFrame({k: data_set[k].values for k in FEATURES}),\n",
    "      y = pd.Series(data_set[LABEL].values),\n",
    "      num_epochs=num_epochs,\n",
    "      shuffle=shuffle)\n",
    "estimator.train(input_fn=input_fn_train, steps=100)\n",
    "\n",
    "def input_fn_eval: # returns x, y\n",
    "    pass\n",
    "metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)\n",
    "def input_fn_predict: # returns x, None\n",
    "    pass\n",
    "predictions = estimator.predict(input_fn=input_fn_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Estimators\n",
    "每一个Estimator的核心（不管是pre-made还是custom）都是model function，\n",
    "是一个建立training, evaluation, 和prediction的方法。  \n",
    "当用 pre-made Estimator的时候，别人已经implemented the model function。\n",
    "在custom Estimator的时候，必须要自己写一个方程  \n",
    "步骤：\n",
    "<ul>\n",
    "    <li>Instantiate an Estimator</li>\n",
    "    <li>Construct a custom model function</li>\n",
    "    <li>Configure a neural network using tf.feature_column and tf.layers</li>\n",
    "    <li>Choose an appropriate loss function from tf.losses</li>\n",
    "    <li>Define a training op for your model</li>\n",
    "    <li>Generate and return predictions</li>\n",
    "</ul>\n",
    "\n",
    "## Recommended workflow\n",
    "建议流程：\n",
    "1. 假设一个合适的pre-made Estimator存在，用这个来创建你的第一个模型，并且用这个结果来建立baseline。\n",
    "2. 创建和检测（build and test）你所有的pipeline，包含在这个pre-made Estimator数据的完整性和可靠性(integrity and reliability)\n",
    "3. 如果有可供选择用来替换的pre-made Estimator是存在的，运行这个pre-made Estimator创建了的最好结果。\n",
    "4. 可能的情况下，可以通过你自己自定义custom Estimator来提升你的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn = tf.estimator.Estimator(model_fn=model_fn, params=model_params)\n",
    "<ul>\n",
    "    <li><b>model_fn:</b> A function object that contains all the aforementioned logic to support training, evaluation, and prediction. You are responsible for implementing that functionality. The next section, Constructing the model_fn covers creating a model function in detail.</li>\n",
    "\n",
    "<li><b>params:</b> An optional dict of hyperparameters (e.g., learning rate, dropout) that will be passed into the model_fn.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "# Import urllib\n",
    "from six.moves import urllib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(train_data, test_data, predict_data):\n",
    "    \"\"\"Maybe downloads training data and returns train and test file names.\"\"\"\n",
    "    if train_data:\n",
    "        train_file_name = train_data\n",
    "    else:\n",
    "        train_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "        urllib.request.urlretrieve(\n",
    "            \"http://download.tensorflow.org/data/abalone_train.csv\",\n",
    "            train_file.name)\n",
    "        train_file_name = train_file.name\n",
    "        train_file.close()\n",
    "        print(\"Training data is downloaded to %s\" % train_file_name)\n",
    "\n",
    "    if test_data:\n",
    "        test_file_name = test_data\n",
    "    else:\n",
    "        test_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "        urllib.request.urlretrieve(\n",
    "            \"http://download.tensorflow.org/data/abalone_test.csv\", test_file.name)\n",
    "        test_file_name = test_file.name\n",
    "        test_file.close()\n",
    "        print(\"Test data is downloaded to %s\" % test_file_name)\n",
    "\n",
    "    if predict_data:\n",
    "        predict_file_name = predict_data\n",
    "    else:\n",
    "        predict_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "        urllib.request.urlretrieve(\n",
    "            \"http://download.tensorflow.org/data/abalone_predict.csv\",\n",
    "            predict_file.name)\n",
    "        predict_file_name = predict_file.name\n",
    "        predict_file.close()\n",
    "        print(\"Prediction data is downloaded to %s\" % predict_file_name)\n",
    "\n",
    "    return train_file_name, test_file_name, predict_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating an estimator\n",
    "# pre-made estimator\n",
    "my_nn = tf.estimator.DNNClassifier(feature_columns=[age, height, weight],\n",
    "                                   hidden_units=[10, 10, 10],\n",
    "                                   activation_fn=tf.nn.relu,\n",
    "                                   dropout=0.2,\n",
    "                                   n_classes=3,\n",
    "                                   optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom estimator\n",
    "nn = tf.estimator.Estimator(model_dn=model_fn,params=model_params)# 两个参数\n",
    "def model_fn(features, labels, mode, params):\n",
    "   # Logic to do the following:\n",
    "   # 1. Configure the model via TensorFlow operations\n",
    "   # 2. Define the loss function for training/evaluation\n",
    "   # 3. Define the training operation/optimizer\n",
    "   # 4. Generate predictions\n",
    "   # 5. Return predictions/loss/train_op/eval_metric_ops in EstimatorSpec object\n",
    "    return EstimatorSpec(mode, predictions, loss, train_op, eval_metric_ops)\n",
    "\n",
    "def my_input_fn():\n",
    "\n",
    "    # Preprocess your data here...\n",
    "\n",
    "    # ...then return 1) a mapping of feature columns to Tensors with\n",
    "    # the corresponding feature data, and 2) a Tensor containing labels\n",
    "    return feature_cols, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_fn接收三个参数：\n",
    "<ul>\n",
    "    <li><b>feature: </b> dict：包含特征，通过input_fn传递给model</li>\n",
    "    <li><b>labels: </b>Tensor：包含labels, 通过input_fn传递给model</li>\n",
    "    <li><b>mode: </b>tf.estimator.ModeKeys的string值代表会工作一下那个model_fn:</li>\n",
    "<p>\n",
    "    input_fn见最上面的说明\n",
    "</p>\n",
    "    <ul>\n",
    "        <li> tf.estimator.ModeKeys.TRAIN：model_fn执行training mode，通过train()调用</li>\n",
    "        <li> tf.estimator.ModeKeys.EVAL: model_fn执行evaluation mode,通过evaluate()调用</li>\n",
    "        <li> tf.estimator.ModeKeys.PREDICT: model_fn执行predict mode，通过predict()调用</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "model_fn 也可以接收params参数（包含一个hyperparameters的字典），用来训练  \n",
    "函数的主体执行以下功能：\n",
    "<ul>\n",
    "    <li>配置模型:集模型的结构</li>\n",
    "    <li>定义损失函数</li>\n",
    "    <li>定义训练的操作，和optimizer的算法来让损失函数变小</li>\n",
    "</ul>\n",
    "<b>return:</b>  \n",
    "返回tf.estimator.EstimatorSpec 对象，包含以下值：  \n",
    "__new__(\n",
    "    cls,  \n",
    "    mode,  \n",
    "    predictions=None,  \n",
    "    loss=None,  \n",
    "    train_op=None,  \n",
    "    eval_metric_ops=None,  \n",
    "    export_outputs=None,  \n",
    "    training_chief_hooks=None,  \n",
    "    training_hooks=None,  \n",
    "    scaffold=None,  \n",
    "    evaluation_hooks=None  \n",
    ")\n",
    "<ul>\n",
    "    <li><b>mode(required) </b>这个model运行的mode，一般而言会在model_fn中返回mode参数</li>\n",
    "    <ul>\n",
    "        <li>For <b>mode == ModeKeys.TRAIN: </b>required fields are loss and train_op.</li>\n",
    "        <li>For <b>mode == ModeKeys.EVAL: </b>required field is loss.</li>\n",
    "        <li>For <b>mode == ModeKeys.PREDICT: </b>required fields are predictions.</li>\n",
    "    </ul>\n",
    "    <li><b>predictions(required in PREDICT model) </b>dict：将选择的名字于model预测值得Tensor匹配 \n",
    "        e.g. python predictions = {\"results\":tensor_of_predictions}  \n",
    "        PREDICT mode,在EstimatorSpec中得dict会通过predict()返回\n",
    "    </li>\n",
    "    <li><b>loss(required in EVAL and TRAIN mode) </b>A Tensor 包含一个标量损失值</li>\n",
    "    <li><b> train_op (required only in TRAIN mode) </b>An Op that runs one step of training.</li>\n",
    "    <li><b> eval_metric_ops (optional) </b>dict: name/value匹配特定得metrics可以被模型在EVAL模型中计算，name是选择得metric，value是计算结果，tf.metrics模块提供一些普遍的已经被定义好得函数  \n",
    "        e.g. 一下的评估中包含一个‘accuracy'的metric使用tf.metrics.accuracy计算：  \n",
    "      python eval_metric_ops = {'accuracy': tf.metrics.accuracy(labels,predictions)}  \n",
    "        如果不用这个参数，那么在evaluation的时候只有loss会被计算\n",
    "    </li>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Configuring a neural network with tf.feature_column and tf.layers\n",
    "<b>model construction +predictions</b>  \n",
    "neural network构建包含三个部分：input layer, hidden layers和output layer  \n",
    "\n",
    "input layer：包含一些列的nodes接收feature data(通过features参数传递到model_fn中)当features包含n-dimension Tensor(所有的特征数据),那么这层就作为input layer。  \n",
    "features： dict of feature_columns，通过input function传递到model中，可以通过tf.feature_column.input_layer函数将其转换成input_layer Tensor。  \n",
    "e.g. : input_layer = tf.feature_column.input_layer(features=features,feature_columns=[age, height, weight])  \n",
    "input_layer()包含两个required参数：\n",
    "<ul>\n",
    "    <li><b>features </b>a mapping 从字符串keys到包含相对应的feature data Tensor。这就是传递到model_fn中的feature参数</li> \n",
    "    <li><b>feature_columns </b>a list 包含所有的模型中的FeatureColumns，例如以上例子中的age, height, weight</li>\n",
    "</ul>\n",
    "<p>\n",
    "    input layer通过activation function(对前一层输出的数据进行非线性转换)和hidden layers连接，hidden layer的最后一层连接到输出层，model的最后一层，tf.layers提供tf.layers.dense方程来构建全连接层。activation通过activation参数进行控制。activation其中一些选择有：\n",
    "    <ul>\n",
    "        <li><b>tf.nn.relu </b>e.g. 创建layer units nodes全连接到前一层的input_layer用relu：  \n",
    "            python hidden_layer= tf.layers.dense(inputs=input_layer,units=10,activation=tf.nn.relu)\n",
    "        </li>\n",
    "        <li><b>tf.nn.relu6 </b>计算方法：min(max(features, 0), 6)  \n",
    "            e.g. python second_hidden_layer = tf.layers.dense( inputs=hidden_layer, units=20, activation=tf.nn.relu)\n",
    "        </li>\n",
    "        <li><b>None </b> 不连接activate function，只有linear transformation  \n",
    "            python output_layer = tf.layers.dense( inputs=second_hidden_layer, units=3, activation=None)\n",
    "        </li>\n",
    "    </ul>\n",
    "其他可能选择：  \n",
    "output_layer = tf.layers.dense(inputs=second_hidden_layer,\n",
    "                               units=10,\n",
    "                               activation_fn=tf.sigmoid)  \n",
    "上面的code船舰了一个output_layer，通过一个sigmoid激活函数全连接到second_hidden_layer\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining loss for the model\n",
    "model_fn返回的EstimatorSpec必须要包含loss：a Tensor(表示loss值)  \n",
    "tf.losses模块提供convenience functions计算不同metrics中的loss：\n",
    "<ul>\n",
    "    <li><b>absolute_difference(labels, predictions) </b>计算loss用absolute-difference方程计算(L1 loss)</li>\n",
    "    <li><b>log_loss(labels, predictions) </b>计算loss用logistic loss formula计算(一般用logistic regression)</li>\n",
    "    <li><b>mean_squared_error(labels, predictions) </b>(MSE;用L2 loss)</li>\n",
    "</ul>\n",
    "以下例子中的model_fn用mean_squared_error()计算  \n",
    "<p>\n",
    "    补充的用来evaluation的metrics加到eval_metric_ops字典当中，以下code定义rmse metric，用来在model predictions时计算root mean sqaured error。labels tensor需要转换到float64来匹配predictions tensor的数据类型（包含实数）：  \n",
    "    eval_metric_ops = {\"rmse\":tf.metrics.root_mean_squared_error(  \n",
    "         tf.cast(lables,tf.float64),predictions))\n",
    "\n",
    "### Defining the training op for the model\n",
    "training op定义优化算法。一般而言，在训练的时候，目标是最小化loss。一个简单的来创建training op的方法是实例化tf.train.Optimizer的子类并且调用它的minimize方法\n",
    "<p>\n",
    "    以下用例当中learning_rate传送到function的param参数当中，使用gradient descent optimizer。对于global_step，采用convenience function tf.train.get_global_step来生成一个整数变量：  \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=params[\"learning_rate\"])  \n",
    "    training_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 完整的model_fn\n",
    "def model_fn(features,labels,mode,params):\n",
    "    \"\"\"Model function for Estimator.\"\"\"\n",
    "\n",
    "  # Connect the first hidden layer to input layer\n",
    "  # (features[\"x\"]) with relu activation\n",
    "    first_hidden_layer = tf.layer.dense(feature['x'],10,activation=tf.nn.relu)\n",
    "    \n",
    "    # Connect the second hidden layer to first hidden layer with relu\n",
    "    second_hidden_layer = tf.layers.dense(first_hidden_layer,10,activation=tf.nn.relu)\n",
    "    \n",
    "    # Connect the output layer to second hidden layer (no activation fn)\n",
    "    output_layer = tf.layers.dense(second_hidden_layer,1)\n",
    "    \n",
    "    # Reshape output layer to 1-dim Tensor to return predictions\n",
    "    predictions = tf.reshape(output_layer,[-1])\n",
    "    predictions_dict = {\"age\":predictions}\n",
    "    \n",
    "    # Calculate loss using mean squared error\n",
    "    loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "    \n",
    "    # calculate root mean squared error as additional eval metric\n",
    "    eval_metric_ops = {\n",
    "        \"rmse\": tf.metrics.root_mean_squared_error(\n",
    "        tf.cast(labels, tf.float64), predictions)\n",
    "    }\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=param[\"learning_rate\"])\n",
    "    train_op = optimizer.minimize(loss=loss,global_step=tf.train_get_global_step())\n",
    "    \n",
    "    # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions = predictions\n",
    "        loss=loss,\n",
    "        train_op=train_op\n",
    "        eval_metric_ops=eval_metric_ops\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "  # Load datasets\n",
    "    abalone_train, abalone_test, abalone_predict = maybe_download(\n",
    "    FLAGS.train_data, FLAGS.test_data, FLAGS.predict_data)\n",
    "\n",
    "  # Training examples\n",
    "    training_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "      filename=abalone_train, target_dtype=np.int, features_dtype=np.float64)\n",
    "\n",
    "  # Test examples\n",
    "    test_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "      filename=abalone_test, target_dtype=np.int, features_dtype=np.float64)\n",
    "\n",
    "  # Set of 7 examples for which to predict abalone ages\n",
    "    prediction_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "      filename=abalone_predict, target_dtype=np.int, features_dtype=np.float64)\n",
    "    \n",
    "    # Set model params\n",
    "    model_params = {\"learning_rate\": LEARNING_RATE}\n",
    "    \n",
    "    # Instantiate Estimator\n",
    "    nn = tf.estimator.Estimator(model_fn=model_fn,params=model_params)\n",
    "    \n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\":np.array(training_set.data)},\n",
    "        y=np.array(training_set.target),\n",
    "        num_epochs=None,\n",
    "        shuffle=True)\n",
    "    \n",
    "    # train\n",
    "    nn.train(input_fn=train_input_fn,steps=5000)\n",
    "    \n",
    "    # score_accuracy\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\":np.array(test_set.data)},\n",
    "        y=np.array(test_set.target),\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    \n",
    "    ev = nn.evaluate(input_fn=test_input_fn)\n",
    "    print(\"Loss: %s\" % ev[\"loss\"])\n",
    "    print(\"Root Mean Squared Error: %s\" % ev[\"rmse\"])\n",
    "    \n",
    "    # print out predictions\n",
    "    predict_input_fn=tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\":prediction_set.data},\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    predictions = nn.predict(input_fn=predict_input_fn)\n",
    "    for i, p in enumerate(predictions):\n",
    "        print(\"Prediction %s: %s\" % (i + 1, p[\"ages\"]))\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "    parser.add_argument(\n",
    "      \"--train_data\", type=str, default=\"\", help=\"Path to the training data.\")\n",
    "    parser.add_argument(\n",
    "      \"--test_data\", type=str, default=\"\", help=\"Path to the test data.\")\n",
    "    parser.add_argument(\n",
    "      \"--predict_data\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Path to the prediction data.\")\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Estimators\n",
    "以下文档介绍custom Estimators。这个文档展示了怎么样创建一个自定义的Estimator类似于pre-made Estimator\n",
    "\n",
    "### Pre-made vs custom\n",
    "在如下的图形中显示，pre-made Estimator是__tf.estimator.Estimator__基础类，自定义的Estimator是tf.estimator.Estimator的实例。\n",
    "![Titile](../image/estimator_types.png)\n",
    "Pre-made Estimator是完全已经弄好的。但是有时候，你需要对Estimator的行为有更多的控制，这就是需要自定义custom Estimator的时候。 你可以创建一个自定义的Estimator来做任何事。如果你想hidden layers用一种不寻常的方式连接，那么使用一个custom Estimator。通常情况下，如果你想要一个Estimator为一个特定的问题优化，也可以写一个自定义的Estimator。\n",
    "\n",
    "一个模型函数(model_fn)实现ML算法。pre-made Estimator和自定义的Estimator的区别是:\n",
    "* pre-made Estimator, 别人已经定义好了模型函数\n",
    "* 自定义 Estimator,你必须自己写一个模型函数。\n",
    "\n",
    "你的model function可以实现很多的算法，定义所有的hidden layers和测量方式(metrics)。 类似于输入方程，所有的模型函数必须要接受一个标准的输入参数，返回一组标准的输出值。就像输入方程可以利用Dataset API,模型函数也可以利用 Layers API和Metrics API。\n",
    "\n",
    "让我们查看怎么样使用自定义的Estimator来解决Iris problem。如下显示了我们想要实现的Iris模型：\n",
    "![Title](../image/full_network.png)\n",
    "\n",
    "### Write an Input function\n",
    "和re-made Estimator inplementation一样，custom Estimator也使用相同的输入函数：\n",
    "\n",
    "这个输入方程创建了一个输入管道(input pipeline)产生batches of (features,labels)对，这些features是字典的数据结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trian_input_fn(fetures,labels,batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to dataset\n",
    "    dataset = tf.train.Dataset.from_tensor_slice(dict(features),labels)\n",
    "    \n",
    "    # Shuffle,repeat, and batch the examples\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    \n",
    "    # Return the read end of the pipeline\n",
    "    return dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature columns\n",
    "与之前描述的一样，必须要定义模型的feature columns来指定模型怎么样使用这些feature。在pre-made Estimator和custom Estimator，feature column的定义方式都是一样的。\n",
    "\n",
    "下民的代码为每一个输入的特征创建了一个简单的numeric_column，于是这输入特征的值可以直接作为模型的输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns describe how to use the input\n",
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a model function \n",
    "模型函数会用下面的签名：\n",
    "> def my_model_fn(  \n",
    "     features, # This is batch_features from input_fn  \n",
    "     labels,   # This is batch_labels from input_fn  \n",
    "     mode,     # An instance of tf.estimator.ModeKeys  \n",
    "     params):  # Additional configuration\n",
    "    \n",
    "前两个参数是从输入方程返回的batches of特征和labels，也就是说，features和labels是模型会使用的数据的句柄(handles)。__mode__ 参数预示这这个caller是在请求training，predicting或者evaluation。\n",
    "\n",
    "这个caller将__params__传递到Estimator的构造器。任何传递给构造器的__params__都会一次传递给__model_fn__。在[custom_estimator.py](https://github.com/tensorflow/models/blob/master/samples/core/get_started/custom_estimator.py)当中，接下来的行创造了一个构造器，设定配置这个模型的参数。这个配置的步骤与之前在DNNClassifier一致。\n",
    "\n",
    "要实现一个镜店的模型函数，你必须要做以下的事情：\n",
    "* 定义模型\n",
    "* 确定下面三个mode的额外的计算：\n",
    "    * predict\n",
    "    * Evaluate\n",
    "    * Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "在基本的DNN模型当中，必须要定义以下三个部分:\n",
    "* 一层输入层(input layer)\n",
    "* 一层或者多层隐藏层(hidden layers)\n",
    "* 一层输出层(output layer)\n",
    "\n",
    "#### Define the input layer\n",
    "在__model_fn__中的第一行，调用__tf.feature_column.input_layer__来将特征字典和__feature_columns__转换到模型的输入，代码如下：  \n",
    "示例当中的代码实现了通过feature columns定义的转换，创建了模型的输入层。\n",
    "![Title](../image/input_layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `input_layer` to apply the feature columns,详细说明见上方\n",
    "net = tf.feature_column.input_layer(features,params['feature_columns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Layers\n",
    "Layers API提供了定义所有类型hidden layers的函数，包括了卷积，池化和dropout层。在Iris,我们单纯调用__tf.layers.dense__创建hidden layers，维度有params['hidden_layer']控制。在dense layer，米一个node都与前一层的每个node相连接，相关代码如下。\n",
    "* __tf.layers.dense（）__ 中的__units__参数定义了给定层的输出的neurons个数。\n",
    "* __activation__ 参数定义了激活函数\n",
    "\n",
    "变量__net__表示是网络的当前顶层。在第一次迭代，__net__表明输入层(input layer)。在每一次循环迭代中，__tf.layers.新的一层，利用__net__变量来将前一层的输出作为当前层的输入。\n",
    "    \n",
    "在创建了两层hidden layers之后，构建的网络如下所示。为了简化，下图中并未显示每层全部的units。\n",
    "![Title](add_hidden_layer.png)\n",
    "注意到，__tf.layers.dense__提供了很多其他的能力，包括设置多个正则参数。为了简单，在这个例子中接受默认的其他参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the hidden layers, sized according to the 'hidden_units' param.\n",
    "for units in param['hidden_units']:\n",
    "    net = tf.layers.dense(net, units=units, activation=tf.n.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Layer\n",
    "通过调用__tf.layers.dense__定义输出层，但是不用激活函数，示例如下：\n",
    "\n",
    "这里，__net_表明最后一层hidden layer。因此，这个网络的连接如下图所示：\n",
    "![Title](../image/add_logits.png)\n",
    "\n",
    "当定义一个输出层的时候，__units__参数指定输出的个数。所以，通关过设定__units__到__patams['n_classes']，这个模型为每类产生一个输出。输出的每个单元都会包含一个评分(score)或者\"logit\",为Iris对应的相关类计算Setosa, Versicolor, or Virginica。\n",
    "\n",
    "然后，这些logits会通过__tf.n.softmax__函数转换成概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute logits (1 per class).\n",
    "logits = tf.layers.dense(net,params['n_classes'], activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement training, evaluation and preodiction\n",
    "创建模型函数的最后一步是实现prediction，evaluationh和training的分支代码。\n",
    "\n",
    "当有人调用Estimators的train, evaluation或predict方法，模型函数会被唤醒。掉哦那个这个模型函数的签名如下代码所示：\n",
    "> def my_model_fn(  \n",
    "    features, # This is batch_features from input_fn  \n",
    "    labels,   # This is batch_labels from input_fn  \n",
    "    mode,     # An instance of tf.estimator.ModeKeys, see below  \n",
    "    params):  # Additional configuration\n",
    "\n",
    "专注于第三个参数,mode。在下面的表格中显示，当有人调用train,evaluate和predict，Estimator框架让你的模型函数的mode参数的设定如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator method\t|Estimator Mode\n",
    ":-:|:-:\n",
    "train()\t|ModeKeys.TRAIN\n",
    "evaluate()\t|ModeKeys.EVAL\n",
    "predict()\t|ModeKeys.PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，假设你实例化你的自定义Estimator来产生一个对象命名为__classifier__。然后，你做如下的调用：\n",
    "Estimator框架将mode设置为__ModeKeys.TRAIN__调用你的模型函数。\n",
    "\n",
    "你的模型函数必须要提供能够处理所有的mode值得code。对每个mode value，code必须要返回一个tf.estimator.EstimatorSpec得实例，这个包含着调用者要求得信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.estimator.Estimator(...)\n",
    "classifier.train(input_fn=lambda:my_input_fn(FILE_TRAIN,True,500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict\n",
    "当调用Estimator得predict方法得时候，__model_fn__接受__mode=ModeKyes.PREDICT_。在这种情况下，模型函数必须要返回一包含prediction的tf.estimator.EstimatorSpec。\n",
    "\n",
    "这个model必须要被train之后才能够做prediction。这个训练的模型被储存在磁盘被在实例化Estimator创建的__model_dir__目录上。\n",
    "\n",
    "这个model生成prediction的code如下：\n",
    "\n",
    "这个prediction字典包含着你模型运行在prediction mode。\n",
    "![Title](../image/add_predictions.png)\n",
    "\n",
    "__predictions__ 包含下面三个key/value对：\n",
    "* __class_ids__ 包含着class id(0,1,or2)代表模型预测的这个例子中最可能的种类。\n",
    "* __probabilities__ 有三个概率(在这个例子中， 0.02, 0.95, and 0.03)\n",
    "* __logit__ 有原始的logit值(在这个例子中，-1.3, 2.6, and -0.9)\n",
    "\n",
    "调用会通过__tf.estimator.EstimatorSpec__的__predictions__参数返回一个字典。这个Estimator的__predict__方法会生成这些字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions.\n",
    "predicted_classes = tf.argmax(logits,1)\n",
    "if mode==tf.estimator.ModeKys.PREDICT:\n",
    "    predictions={\n",
    "        'class_ids': predicted_classes[:, tf.newaxis],\n",
    "        'probabilities': tf.nn.softmax(logits),\n",
    "        'logits': logits,\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode,predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the loss\n",
    "在training和evaluation都需要计算模型的loss。这就是优化的目标。\n",
    "\n",
    "调用__tf.losses.sparse_sotfmax_cross_entropy__可以计算loss。 这个函数返回的值会很小，大约阶级0，正确类的概率大约接近1。当正确类的概率降低的时候，loss的返回值会逐渐增大。\n",
    "\n",
    "这函数会返回这个batch的平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "loss = tf.losses.sparse_sotfmax_cross_entropy(labels=labels,logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "当调用Estimator的evaluate方法的时候，__model_fn__接收__mode=ModeKeys.EVAL__。在这种情况下，模型函数返回一个包含模型loss和选择的一种或者多种的metrics的__tf.estimator.EstimatorSpec。\n",
    "真实值\n",
    "虽然返回metrics是选择项，但是大部分的自定义的Estimator会返回至少一个metrics。TensorFlow提供一个Metrics模块__tf.metrics__来计算正常的metrics。为了简洁起见，我们只返回精度。__tf.metric.accuracy__函数将预测值与真实值比较，这些真实值来自己输入函数的提供。这个函数要求预测值和真实值有相同的shape。下面是调用__tf.metrics.accuracy__示例如1:\n",
    "\n",
    "evaluation返回的__EstimatorSpec__一般包含下面的信息：\n",
    "* __loss__ ， 模型的loss\n",
    "* _eval_metric_ops__ , 可选择的metric字典\n",
    "\n",
    "因此，我们创造一个字典包含单一的metric。如果需要计算另外的metrics, 需要添加额外的key/value对到同一个字典。然后，将这个字典传递到__tf.estimator.EstimatorSpec__的__eval_metric_ops__参数中，示例如2：\n",
    "\n",
    "__tf.sumary.scalar__ 在TRAIN和EVAL mode下让accuracy提供给 Tensoboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:\n",
    "# Compute evaluation metircs:\n",
    "accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                               predictions=predicted_classes,\n",
    "                               name='acc_op'\n",
    "                              )\n",
    "# 2:\n",
    "metrics = {'accuracy':accuracy}\n",
    "tf.summary.scalar('accuracy',accuracy[1])\n",
    "\n",
    "if mode==tf.estimator.ModeKyes.EVAL:\n",
    "    return f.estimator.EstimatorSpec(mode,loss=loss,eval_metric_ops=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trai\n",
    "当Estimator调用train方法，__model_fn__调用__mode=ModeKeys.TRAIN。在这个情况下，模型函数返回一个含有loss和training operation的EstimatorSpec。\n",
    "\n",
    "建立一个training operation会要求一个optimizer。 由于是模仿DNNClassifier，其中也是用Adagrad，我们会使用__tf.train.AdagradOptimizer__。__tf.train__包提供了很多其他的optimizer。\n",
    "\n",
    "下面的代码建立了一个optimizer。\n",
    "\n",
    "建立完optimizer之后，通过使用optimizer在loss上面的minimize方法创建training operation。\n",
    "\n",
    "这个minimize方法也会添加__global_step__参数。TensorFLow使用这个参数来计算training steps已经运行过的次数(可以知道怎么样结束训练的次数)。__global_step__是TensorBoard graphs正常工作的必要参数。 简单调用__tf.train.get_global_step__并且将结果传达到__minimize__的__global_step__。下面的代码中详见怎么样训练模型：\n",
    "\n",
    "training返回的EstimatorSpec有余下字段设置(field set)：\n",
    "* __loss__ ， 包含loss函数值\n",
    "* __train_op__ , 执行一个training step.\n",
    "详见代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "\n",
    "train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The custom Estimator\n",
    "自定义Estimator的实例化通过Estimator的基础类，代码如下1所示：\n",
    "\n",
    "这个示例代码中的__params__中的字典和DNNClassifier中的关键参数具有同样的目的。也就是说，__params__字典让你可以不修改__model_fn__而配置Estimator。\n",
    "\n",
    "定义完Estimator之后，其中train，evaluate，predictions和之前一样。如下代码2所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:\n",
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=my_model,\n",
    "    params={\n",
    "        'feature_columns': my_feature_columns,\n",
    "        # Two hidden layers of 10 nodes each.\n",
    "        'hidden_units': [10, 10],\n",
    "        # The model must choose between 3 classes.\n",
    "        'n_classes': 3,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2:\n",
    "# Train the Model.\n",
    "classifier.train(\n",
    "    input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size),\n",
    "    steps=args.train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "可以在TensorBoard中查看training result\n",
    "> \\# Replace PATH with the actual path passed as model_dir  \n",
    "tensorboard --logdir=PATH\n",
    "\n",
    "通过浏览器网址[http://localhost:6006/](http://localhost:6006/) 打开TensorBoard。（最好使用Google浏览器）\n",
    "\n",
    "所有的pre-made Estimator都会自动载入很多信息到TensorBoard。在自定义的Estimator，虽然，TensorBoard只会提供一个默认的日志(a graph of the loss)外加你显示的告诉TensorBoard应该记录的信息。对于你创建的自定义的Estimator，TensorBoard长生一下图片：\n",
    "![Title](../image/accuracy.png)\n",
    "![Title](../image/loss.png)\n",
    "![Title](../image/steps_per_second.png)\n",
    "\n",
    "简单来说，这三个图告诉你：\n",
    "* global_step/sec: 这个表现知识说明模型训练的时候，美妙有多少batches(梯度升级)\n",
    "* loss: 表明loss\n",
    "* accuracy： accuracy由两条线记录：\n",
    "    * eval_metric_ops={'my_accuracy': accuracy},在evaluation的时候\n",
    "    * tf.summary.scalar('accuracy', accuracy[1])，在训练的时候。\n",
    "\n",
    "这些tensorboard的图是传递__global_step__到optimizer的__minimize__方法的主要原因之一。这个模型如果没有这个参数就无法记录x坐标。\n",
    "\n",
    "注意到下面的这些__my_accuracy__和__loss__图:\n",
    "* 黄色的线代表训练时候的结果\n",
    "* 蓝色的点代表evaluation的结果。\n",
    "\n",
    "在训练的时候，summaries(黄色的线)会随着batches的处理周期的记录，这也是为什么这个图会成为跨越x轴的图形。\n",
    "\n",
    "与之不同，evaluation只会在每次调用evaluate的时候产生一个点。这个点包含了当整个evaluation调用的时候的平均值。在一个特定的训练步骤（从单一的checkpoint）,由于是评估整个模型的状态，所以这个图没有任何的宽度。\n",
    "\n",
    "如下图所示，可以通过左边的可选圆圈来选择TensorBoard需要显示的曲线。\n",
    "![Title](../image/select_run.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "虽然pre-made Estimators可以快速的创建一个模型，但是custom Estimators提供更多的领命多。幸运的是，pre-made和custom Estimator使用的是同一个编程模型。在实际当中不同的就是在custom Estimator时，必须要自己协定模型函数。\n",
    "* The [official TensorFlow implementation of MNIST](https://github.com/tensorflow/models/tree/master/official/mnist), which uses a custom estimator.\n",
    "* The [TensorFlow official models repository](https://github.com/tensorflow/models/tree/master/official), which contains more curated examples using custom estimators.\n",
    "* This [TensorBoard video](https://ipv6.google.com/sorry/index?continue=https://youtu.be/eBbEDRsCmv4&q=EhAgAQJQMABL8xm08wdmAu0NGJb3v9MFIhkA8aeDS5SM5bnJ8QGbBcPEMpMT9BcmXiStMgFy), which introduces TensorBoard.\n",
    "* The [Low Level Introduction](https://www.tensorflow.org/programmers_guide/low_level_intro?hl=zh-cn), which demonstrates how to experiment directly with TensorFlow's low level APIs, making debugging easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Estimators from Keras models\n",
    "\n",
    "你可以将现在的Keras models转换到Estimator。 这样就要让Keras模型由进入Estimator的能力，例如分散训练。 调用__tf.keras.estimator.model_to_estimator__如下例所示：\n",
    "\n",
    "注意到上例当中的fature columns的名字和keras estimator的labels都来自与相应的已经编译好的keras模型。例如，__train_input_fn__中输入key的名字可以通过__keras_inception_v3.input_names__来获得，同样的，预测的输出的名字可以通过__keras_inception_v3.output_names__。\n",
    "\n",
    "详细的信息查看 [tf.keras.estimator.model_to_estimator](https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator?hl=zh-cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Keras inception v3 model.\n",
    "keras_inception_v3 = tf.keras.applications.inception_v3.InceptionV3(weights=None)\n",
    "# Compile model with the optimizer, loss, and metrics you'd like to train with.\n",
    "keras_inception_v3.compile(optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metric='accuracy')\n",
    "# Create an Estimator from the compiled Keras model. Note the initial model\n",
    "# state of the keras model is preserved in the created Estimator.\n",
    "est_inception_v3 = tf.keras.estimator.model_to_estimator(keras_model=keras_inception_v3)\n",
    "\n",
    "# Treat the derived Estimator as you would with any other Estimator.\n",
    "# First, recover the input name(s) of Keras model, so we can use them as the\n",
    "# feature column name(s) of the Estimator input function:\n",
    "keras_inception_v3.input_names  # print out: ['input_1']\n",
    "# Once we have the input name(s), we can create the input function, for example,\n",
    "# for input(s) in the format of numpy ndarray:\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"input_1\": train_data},\n",
    "    y=train_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "# To train, we call Estimator's train function:\n",
    "est_inception_v3.train(input_fn=train_input_fn, steps=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
